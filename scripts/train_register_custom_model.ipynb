{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc324be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install model_deploy-0.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ..\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8434bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from importlib.metadata import version\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_deploy.config import ProjectConfig, Tags\n",
    "from model_deploy.models.basic_model import BasicModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a254c25",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS mlops_dev.model_test.training_control (\n",
    "    allow_training BOOLEAN,\n",
    "    updated_on TIMESTAMP\n",
    ");\n",
    "\n",
    "INSERT INTO mlops_dev.model_test.training_control VALUES (true, current_timestamp());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# PATCH BASICMODEL FOR SKLEARN DATASET\n",
    "# -----------------------------------------------------------\n",
    "def patch_basicmodel_for_sklearn():\n",
    "    \"\"\"Patch BasicModel.load_data() and BasicModel.prepare_features() \n",
    "       to use sklearn dataset instead of Delta tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "                # ----------------------------------------------------\n",
    "        # ðŸ” TRAINING CONTROL CHECK\n",
    "        # ----------------------------------------------------\n",
    "        logger.info(\"ðŸ” Checking training control flag in Lakehouse...\")\n",
    "\n",
    "        flag = spark.sql(\"\"\"\n",
    "            SELECT allow_training\n",
    "            FROM mlops_dev.model_test.training_control\n",
    "            ORDER BY updated_on DESC\n",
    "            LIMIT 1\n",
    "        \"\"\").first()[0]\n",
    "\n",
    "        if not flag:\n",
    "            logger.warning(\"â›” Training disabled via training_control table. Skipping training.\")\n",
    "            # Tell pipeline to skip\n",
    "            dbutils.jobs.taskValues.set(\"training_skipped\", \"TRUE\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        logger.info(\"âœ… Training allowed â€” proceeding with dataset loading.\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # LOAD SKLEARN DATASET\n",
    "        # ----------------------------------------------------\n",
    "        logger.info(\"ðŸ”„ Loading sklearn breast cancer dataset...\")\n",
    "\n",
    "        data = load_breast_cancer()\n",
    "        df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        df[\"target\"] = data.target\n",
    "\n",
    "        # ALL numeric features\n",
    "        self.num_features = list(df.columns)\n",
    "        self.num_features.remove(\"target\")\n",
    "\n",
    "        self.cat_features = []   # No categorical features\n",
    "\n",
    "        # Train/Test split\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "        self.train_set = train_df\n",
    "        self.test_set = test_df\n",
    "\n",
    "        self.X_train = train_df[self.num_features]\n",
    "        self.y_train = train_df[\"target\"]\n",
    "        self.X_test = test_df[self.num_features]\n",
    "        self.y_test = test_df[\"target\"]\n",
    "\n",
    "        # Needed for model_improved()\n",
    "        self.eval_data = test_df.copy()\n",
    "\n",
    "        logger.info(\"âœ… sklearn dataset loaded successfully.\")\n",
    "\n",
    "    def prepare_features(self):\n",
    "        logger.info(\"ðŸ”„ No preprocessing needed for sklearn dataset.\")\n",
    "        from lightgbm import LGBMClassifier\n",
    "        self.pipeline = LGBMClassifier(**self.parameters)\n",
    "        logger.info(\"âœ… Pipeline ready using LGBMClassifier.\")\n",
    "\n",
    "    def log_model(self):\n",
    "        logger.info(\"ðŸ“¦ Logging model + parameters + metrics to MLflow...\")\n",
    "\n",
    "        # mlflow.set_experiment(self.experiment_name)\n",
    "        mlflow.set_experiment(\"/Shared/mlops_exp\")\n",
    "\n",
    "        with mlflow.start_run(run_name=\"basic-lgbm\", tags=self.tags) as run:\n",
    "\n",
    "            # â­ save run_id for register_model()\n",
    "            self.run_id = run.info.run_id\n",
    "            dbutils.jobs.taskValues.set(\"candidate_run_id\", self.run_id)\n",
    "            # --- parameters ---\n",
    "            mlflow.log_params(self.config.parameters)\n",
    "\n",
    "            # --- metrics ---\n",
    "            y_pred = self.pipeline.predict(self.X_test)\n",
    "\n",
    "            self.metrics = {\n",
    "                \"f1_score\": float(f1_score(self.y_test, y_pred)),\n",
    "                \"accuracy\": float(accuracy_score(self.y_test, y_pred)),\n",
    "                \"precision\": float(precision_score(self.y_test, y_pred)),\n",
    "                \"recall\": float(recall_score(self.y_test, y_pred)),\n",
    "            }\n",
    "\n",
    "            mlflow.log_metrics(self.metrics)\n",
    "\n",
    "            # --- model artifact ---\n",
    "            logger.info(\"ðŸ“ Logging sklearn LightGBM model...\")\n",
    "            signature = infer_signature(self.X_train, self.pipeline.predict(self.X_train))\n",
    "\n",
    "            self.model_info = mlflow.sklearn.log_model(\n",
    "                sk_model=self.pipeline,\n",
    "                artifact_path=\"model\",\n",
    "                signature=signature,\n",
    "                input_example=self.X_train.iloc[0:1],\n",
    "            )\n",
    "\n",
    "        logger.info(f\"âœ… MLflow logging completed. Run ID: {self.run_id}, Metrics: {self.metrics}\")\n",
    "\n",
    "\n",
    "        # Patch methods\n",
    "    BasicModel.load_data = load_data\n",
    "    BasicModel.prepare_features = prepare_features\n",
    "    BasicModel.log_model = log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2953394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# MANUAL RUN (NO ARGPARSE NEEDED)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "env = \"dev\"  # You can change manually: dev / acc / prd\n",
    "config_path = \"model_config_deploy.yml\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Manual tags for logging\n",
    "tags = Tags(\n",
    "    git_sha=\"manual_run\",\n",
    "    branch=\"manual_run\",\n",
    "    job_run_id=\"manual_run\"#spark.conf.get(\"spark.databricks.job.id\", \"unknown_job_id\")\n",
    ")\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=config_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Patch BEFORE running training\n",
    "patch_basicmodel_for_sklearn()\n",
    "print(config)\n",
    "basic_model = BasicModel(config=config, tags=tags, spark=spark)\n",
    "\n",
    "basic_model.load_data()\n",
    "basic_model.prepare_features()\n",
    "\n",
    "basic_model.train()\n",
    "basic_model.log_model()\n",
    "\n",
    "model_improved = basic_model.model_improved() # Ensure the model alias exists before calling model_improved, or handle the case where it does not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient, register_model\n",
    "client = MlflowClient()\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
    "\n",
    "# Check model visibility\n",
    "try:\n",
    "    print(\"Registered model:\", client.get_registered_model(\"mlops_dev.model_test.model_deploy\"))\n",
    "except Exception as e:\n",
    "    print(\"Registered model not found or error:\", e)\n",
    "\n",
    "# Check alias (if supported)\n",
    "try:\n",
    "    mv = client.get_model_version_by_alias(\"mlops_dev.model_test.model_deploy\", \"latest-model\")\n",
    "    print(\"Alias latest-model ->\", mv.version)\n",
    "except Exception as e:\n",
    "    print(\"Alias lookup error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------\n",
    "# # If model is better â†’ register + wrap\n",
    "# # -----------------------------------------------------------\n",
    "\n",
    "# if model_improved:\n",
    "#     # Register version\n",
    "#     new_version = basic_model.register_model()\n",
    "\n",
    "#     pkg_version = version(\"model_deploy\")  # your package name\n",
    "\n",
    "#     pyfunc_model_name = (\n",
    "#         f\"{config.catalog_name}.{config.schema_name}.model_deploy_custom\"\n",
    "#     )\n",
    "\n",
    "#     # code_paths = [\n",
    "#     #     f\"./artifacts/.internal/model_deploy-{pkg_version}-py3-none-any.whl\"\n",
    "#     # ]\n",
    "#     import model_deploy, os\n",
    "#     code_dir = os.path.dirname(model_deploy.__file__)\n",
    "#     wrapper = MarvelModelWrapper()\n",
    "\n",
    "#     latest_version = wrapper.log_register_model(\n",
    "#         wrapped_model_uri=f\"{basic_model.model_info.model_uri}\",\n",
    "#         pyfunc_model_name=pyfunc_model_name,\n",
    "#         experiment_name=config.experiment_name_custom,\n",
    "#         input_example=basic_model.X_test[0:1],\n",
    "#         tags=tags,\n",
    "#         code_paths=[code_dir],\n",
    "#     )\n",
    "\n",
    "#     logger.info(f\"ðŸŸ¢ New model registered â†’ version {latest_version}\")\n",
    "\n",
    "#     dbutils.jobs.taskValues.set(key=\"model_version\", value=latest_version)\n",
    "#     dbutils.jobs.taskValues.set(key=\"model_updated\", value=1)\n",
    "\n",
    "# else:\n",
    "#     logger.info(\"ðŸ”´ Model did NOT improve â†’ not registering\")\n",
    "#     dbutils.jobs.taskValues.set(key=\"model_updated\", value=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
